"""
Fact Checker Agent:
This module defines a Fact Checker agent that verifies the GROUNDEDNESS
of responses generated by other agents using Azure AI Content Safety.
"""
from pydantic import BaseModel, Field

from agent_framework import ChatAgent

from agents.base import chat_client
from safety.groundedness_check import check_agent_groundedness


class GroundednessResult(BaseModel):
    is_grounded: bool = Field(
        description="Whether the agent response is grounded in the source documents")
    grounding_score: float = Field(
        description="Grounding score from 1 (least grounded) to 5 (most grounded)")
    reason: str = Field(
        description="Explanation for the grounding score")


fact_checker = ChatAgent(
    chat_client=chat_client,
    name="fact_checker",
    instructions="""
You are a Fact Checker agent. Your role is to verify whether the retriever's
enriched purchase order (RetrievedPO) is grounded in the raw Azure Search documents.

Workflow:
1. You receive a RetrievedPO object with a retrieval_evidence field containing raw search docs.
2. Serialize the ENTIRE RetrievedPO to clean JSON (use model_dump_json() mentally).
3. Extract retrieval_evidence list from the input - pass it UNCHANGED as source_docs.
4. Create a query string: "Order for {customer_name} - {item count} items"
5. Call check_agent_groundedness with:
   • agent_response = the full RetrievedPO JSON from step 2
   • source_docs = retrieval_evidence list from step 3 (AS IS, no modification)
   • query = the summary from step 4
6. Map the tool's output directly to GroundednessResult:
   • is_grounded ← groundedness_result
   • grounding_score ← groundedness_score  
   • reason ← groundedness_reason

CRITICAL: Pass retrieval_evidence AS IS to the tool. Do NOT modify, filter, or reformat it.
Always call the tool - never skip it or fabricate results.
""",
    tools=[
        check_agent_groundedness,
    ],
    response_format=GroundednessResult,
)